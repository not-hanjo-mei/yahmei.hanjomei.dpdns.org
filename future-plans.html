<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Future Plans - YAH Mei</title>
  <meta name="description" content="Upcoming features and enhancements for YAH Mei">

  <!-- Favicon -->
  <link rel="icon" href="img/favicon.svg" type="image/svg+xml">

  <!-- Material Icons -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Product+Sans:400,500,700&display=swap">

  <!-- Cascadia Mono for dot art -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fontsource/cascadia-mono/index.css">

  <!-- Our CSS -->
  <link rel="stylesheet" href="css/reset.css">
  <link rel="stylesheet" href="css/variables.css">
  <link rel="stylesheet" href="css/fonts.css">
  <link rel="stylesheet" href="css/main.css">
  <link rel="stylesheet" href="css/hero.css">
  <link rel="stylesheet" href="css/navigation.css">
  <link rel="stylesheet" href="css/components.css">
  <link rel="stylesheet" href="css/buttons.css">
  <link rel="stylesheet" href="css/feature-cards.css">
  <link rel="stylesheet" href="css/responsive.css">
</head>
<body>
  <!-- Navigation Rail (Sidebar) -->
  <nav class="nav-rail">
    <div class="logo-container">
      <span class="material-symbols-outlined">smart_toy</span>
    </div>

    <a href="index.html" class="nav-icon" data-section="home">
      <span class="material-symbols-outlined">home</span>
      <span class="nav-label">Home</span>
    </a>

    <a href="index.html#about" class="nav-icon" data-section="about">
      <span class="material-symbols-outlined">person</span>
      <span class="nav-label">About</span>
    </a>

    <div class="nav-rail-divider"></div>

    <a href="how-it-works.html" class="nav-icon" data-section="how">
      <span class="material-symbols-outlined">code</span>
      <span class="nav-label">How It Works</span>
    </a>

    <a href="future-plans.html" class="nav-icon active" data-section="future">
      <span class="material-symbols-outlined">rocket_launch</span>
      <span class="nav-label">Future Plans</span>
    </a>

    <a href="disclaimer.html" class="nav-icon" data-section="disclaimer">
      <span class="material-symbols-outlined">gavel</span>
      <span class="nav-label">Disclaimer</span>
    </a>
  </nav>

  <!-- Bottom Navigation (for mobile) -->
  <nav class="bottom-nav">
    <div class="nav-container">
      <a href="index.html" class="nav-icon" data-section="home">
        <span class="material-symbols-outlined">home</span>
        <span class="nav-label">Home</span>
      </a>
      <a href="index.html#about" class="nav-icon" data-section="about">
        <span class="material-symbols-outlined">person</span>
        <span class="nav-label">About</span>
      </a>
      <a href="how-it-works.html" class="nav-icon" data-section="how">
        <span class="material-symbols-outlined">code</span>
        <span class="nav-label">How</span>
      </a>
      <a href="future-plans.html" class="nav-icon active" data-section="future">
        <span class="material-symbols-outlined">rocket_launch</span>
        <span class="nav-label">Future</span>
      </a>
      <a href="disclaimer.html" class="nav-icon" data-section="disclaimer">
        <span class="material-symbols-outlined">gavel</span>
        <span class="nav-label">Legal</span>
      </a>
    </div>
  </nav>

  <!-- Main Content -->
  <main class="main-content">
    <!-- Hero Section -->
    <section id="future" class="section">
      <div class="container">
        <h1 class="display-medium">Future Plans</h1>
        <p class="headline-small">What's next for YAH Mei? Exciting features on the horizon!</p>
      </div>
    </section>

    <!-- Project Plan Section -->
    <section class="section">
      <div class="container">
        <h2 class="headline-medium">Project Plan: Multilingual REALITY Livestreaming Bot</h2>
        <p class="body-large">
          YAH Mei is evolving into a comprehensive livestreaming solution. Here's our detailed project plan:
        </p>

        <!-- Core Architecture -->
        <div class="roadmap-container">
          <div class="feature-card">
            <div class="feature-icon">
              <span class="material-symbols-outlined">architecture</span>
              <h3 class="title-large">Core Architecture</h3>
            </div>
            <span class="feature-status">In Development</span>
            <p class="body-medium pre-line">
              Low-Cost Hybrid Model:
              • PC (Controller + REALITY Environment)
              • 2 Dedicated Android Phones (Processing)
              • Cloud AI Services (Core Intelligence)
            </p>
          </div>
        </div>

        <!-- Components & Roles -->
        <h2 class="headline-medium">Components & Roles</h2>
        
        <!-- PC Component -->
        <div class="roadmap-container">
          <div class="feature-card">
            <div class="feature-icon">
              <span class="material-symbols-outlined">computer</span>
              <h3 class="title-large">PC (Primary Controller & REALITY Environment)</h3>
            </div>
            <span class="feature-status">In Development</span>
            <p class="body-medium pre-line">
              • <strong>OS:</strong> User's primary OS (Windows/Linux/Mac)
              • <strong>Core Script:</strong> Python 3.x with LangChain framework for orchestration and AI interaction
              • <strong>GUI Interface:</strong> Python-based dashboard using PyQt5 or Tkinter for:
                  ◦◦◦◦ Real-time status monitoring
                  ◦◦◦◦ Audio level visualization and control
                  ◦◦◦◦ Conversation history display
                  ◦◦◦◦ Manual override controls
                  ◦◦◦◦ System configuration panel
              • <strong>REALITY Environment:</strong> Android Emulator running REALITY App and ADBKeyboard
              • <strong>Audio Routing:</strong> Voicemeeter for audio capture and playback management
              • <strong>RAG Storage:</strong> Local ChromaDB for conversation history/memory
              • <strong>AI API Client:</strong> OpenAI Python library for cloud and local services
              • <strong>ADB Control:</strong> Python subprocess module for emulator commands
              • <strong>Viewer Comment Reading:</strong> WebSocket connection to REALITY App
            </p>
          </div>
        </div>

        <!-- Phone 1 Component -->
        <div class="roadmap-container">
          <div class="feature-card">
            <div class="feature-icon">
              <span class="material-symbols-outlined">smartphone</span>
              <h3 class="title-large">Phone 1 (ASR Server)</h3>
            </div>
            <span class="feature-status">In Development</span>
            <p class="body-medium pre-line">
              • <strong>Hardware:</strong> Dedicated Android phone
              • <strong>Environment:</strong> chroot Linux environment (Debian/Ubuntu ARM)
              • <strong>ASR Engine:</strong> faster-whisper (using tiny or base multilingual models)
              • <strong>Server:</strong> Python web server exposing an OpenAI-compatible API endpoint
            </p>
          </div>
        </div>

        <!-- Phone 2 Component -->
        <div class="roadmap-container">
          <div class="feature-card">
            <div class="feature-icon">
              <span class="material-symbols-outlined">smartphone</span>
              <h3 class="title-large">Phone 2 (TTS Server)</h3>
            </div>
            <span class="feature-status">In Development</span>
            <p class="body-medium pre-line">
              • <strong>Hardware:</strong> Dedicated Android phone
              • <strong>Environment:</strong> chroot Linux environment
              • <strong>TTS Engine:</strong> MeloTTS with custom-trained EN/ZH/JA voice models
              • <strong>Server:</strong> Python web server exposing an OpenAI-compatible API endpoint
            </p>
          </div>
        </div>

        <!-- Cloud Services Component -->
        <div class="roadmap-container">
          <div class="feature-card">
            <div class="feature-icon">
              <span class="material-symbols-outlined">cloud</span>
              <h3 class="title-large">Cloud Services</h3>
            </div>
            <span class="feature-status">In Development</span>
            <p class="body-medium">
              AI Models providing LLM and Embedding capabilities, accessed via an OpenAI-compatible API endpoint (e.g., Google Gemini backend configured for compatibility).
            </p>
          </div>
        </div>

        <!-- Workflow Section -->
        <h2 class="headline-medium">Workflow</h2>
        <div class="roadmap-container">
          <div class="feature-card">
            <div class="feature-icon">
              <span class="material-symbols-outlined">schema</span>
              <h3 class="title-large">System Workflow</h3>
            </div>
            <span class="feature-status">In Development</span>
            <p class="body-medium pre-line">
              1. <strong>Input:</strong>
                 • Voice: Collaborators speak within the REALITY App
                 • Text: Viewers post text comments in the REALITY App
              
              2. <strong>Audio Capture:</strong> Emulator audio captured by Voicemeeter
              
              3. <strong>ASR Request:</strong> Core Script sends audio to ASR Server on Phone 1
              
              4. <strong>ASR Processing:</strong> Phone 1 transcribes audio and returns text
              
              5. <strong>Viewer Comment Reading:</strong> Core Script reads latest viewer comments
              
              6. <strong>LangChain Processing (PC):</strong>
                 • Receives ASR transcription and viewer comments
                 • Calls Cloud Embedding API to generate query vector
                 • Queries local ChromaDB for relevant context
                 • Constructs complete prompt with all information
                 • Calls Cloud LLM API with the complete prompt
                 • Receives text response and detects language
              
              7. <strong>TTS Request:</strong> Core Script sends text to TTS Server on Phone 2
              
              8. <strong>TTS Processing:</strong> Phone 2 generates audio from text
              
              9. <strong>Audio Output:</strong> Core Script plays audio through Voicemeeter
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="footer-content">
          <div class="footer-info">
            <h2 class="title-large">YAH Mei</h2>
            <p class="body-medium">Remember AI is only a tool. You can't just press that darn 'Generate' button. You have to think!</p>
          </div>
          <div class="social-links">
            <a href="https://hanjomei.dpdns.org/" aria-label="Main Website">
              <span class="material-symbols-outlined">home</span>
            </a>
          </div>
        </div>
      </div>
    </footer>
  </main>

  <!-- JavaScript Files -->
  <script src="js/dotart.js"></script>
  <script src="js/main.js"></script>
</body>
</html>
